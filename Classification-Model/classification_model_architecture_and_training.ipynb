{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,Dense,GlobalAveragePooling2D,BatchNormalization, Activation, Dropout,Input, Lambda\n",
    "from tensorflow.keras.regularizers import l2,L1\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_SIZE = 512\n",
    "NUM_CLASSES = 4\n",
    "# Set batch size\n",
    "BATCH_SIZE = 8\n",
    "# Set learning rate\n",
    "LR = 1e-5\n",
    "# Set epoch\n",
    "EPOCHS = 70\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading, Preprocessing, and Generator Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading CSV file, adjusting image names, defining directories, preprocessing images, setting up a custom data generator class, and preparing data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file with ground truth\n",
    "labels_df = pd.read_csv('publish.csv')\n",
    "\n",
    "# Adjust image names in labels_df to match the format without extension\n",
    "labels_df['Image Name'] = labels_df['Image Name'].str.replace('.tif', '')\n",
    "\n",
    "# Define paths to the directories with training and test images\n",
    "train_image_dir = 'masks/train'\n",
    "test_image_dir = 'masks/test'\n",
    "\n",
    "# Get a list of all image files in the directories without extensions\n",
    "train_image_files = [f[:-4] for f in os.listdir(train_image_dir) if f.endswith('.png')]\n",
    "test_image_files = [f[:-4] for f in os.listdir(test_image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Filter labels_df to include only the images that exist in train_image_dir and test_image_dir\n",
    "train_df = labels_df[labels_df['Image Name'].isin(train_image_files)].reset_index(drop=True)\n",
    "test_df = labels_df[labels_df['Image Name'].isin(test_image_files)].reset_index(drop=True)\n",
    "\n",
    "# Add the file extension .png to Image Name in train_df and test_df\n",
    "train_df['Image Name'] = train_df['Image Name'].apply(lambda x: os.path.join(train_image_dir, x + '.png'))\n",
    "test_df['Image Name'] = test_df['Image Name'].apply(lambda x: os.path.join(test_image_dir, x + '.png'))\n",
    "\n",
    "# Optional: Save the filtered DataFrames to new CSV files if needed\n",
    "train_df.to_csv('filtered_train_data.csv', index=False)\n",
    "test_df.to_csv('filtered_test_data.csv', index=False)\n",
    "\n",
    "# Function to read and preprocess an image\n",
    "def load_and_preprocess_image(filepath):\n",
    "    image = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    image = np.expand_dims(image, axis=-1)  # Add channel dimension for grayscale\n",
    "    image = np.array(image, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "# Define DataGenerator class\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, images, labels, batch_size, target_size, n_classes, shuffle=True):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.images) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_images = self.images[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return np.array(batch_images), tf.keras.utils.to_categorical(batch_labels, num_classes=self.n_classes)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            indices = np.arange(len(self.images))\n",
    "            np.random.shuffle(indices)\n",
    "            self.images = self.images[indices]\n",
    "            self.labels = self.labels[indices]\n",
    "\n",
    "# Define target size\n",
    "target_size = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "# Create generators\n",
    "train_generator = DataGenerator(\n",
    "    np.array([load_and_preprocess_image(filename) for filename in train_df['Image Name']]),\n",
    "    train_df['MF-Grade'].values,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    target_size=target_size, \n",
    "    n_classes=NUM_CLASSES\n",
    ")\n",
    "test_generator = DataGenerator(\n",
    "    np.array([load_and_preprocess_image(filename) for filename in test_df['Image Name']]),\n",
    "    test_df['MF-Grade'].values,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=target_size,\n",
    "    n_classes=NUM_CLASSES,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Now you can use train_generator and test_generator for model training\n",
    "x_train, y_train = [], []\n",
    "for batch_x, batch_y in train_generator:\n",
    "    x_train.extend(batch_x)\n",
    "    y_train.extend(batch_y)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(len(x_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "x_test, y_test = [], []\n",
    "for batch_x, batch_y in test_generator:\n",
    "    x_test.extend(batch_x)\n",
    "    y_test.extend(batch_y)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "print(\"Number of test samples:\", len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting a pie chart to visualize the distribution of classes in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.sum(y_train, axis=0)\n",
    "\n",
    "# Define class labels\n",
    "class_labels = ['mf0', 'mf1', 'mf2', 'mf3']  # Update these labels based on your actual classes\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.figure(figsize=(14, 6))\n",
    "colors = ['#4285f4', '#ea4335', '#fbbc05', '#34a853']\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.pie(class_counts, labels=class_labels, colors=colors, autopct='%.1f%%', explode=(0.025, 0.025, 0.025, 0.025), startangle=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True):\n",
    "    initializer = he_normal(seed=90)\n",
    "    x = Conv2D(filters=n_filters, \n",
    "               kernel_size=(kernel_size, kernel_size), \n",
    "               kernel_initializer=initializer, \n",
    "               padding='same',\n",
    "               kernel_regularizer=l2(1e-5))(input_tensor)  # Adjust regularization strength as needed\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(num_classes=4):\n",
    "    # Define input layer for grayscale images\n",
    "    inputs = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1), name=\"input_image\")\n",
    "    \n",
    "    # Convert grayscale to RGB by duplicating the channel\n",
    "    rgb_inputs = Lambda(lambda x: tf.image.grayscale_to_rgb(x))(inputs)\n",
    "    \n",
    "    encoder = Xception(input_tensor=rgb_inputs, include_top=False, weights=\"imagenet\")\n",
    "    \n",
    "    # Freeze the layers \n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    encoder_output = encoder.layers[-1].output\n",
    "    # Additional convolutional layers\n",
    "    x = conv2d_block(encoder_output,n_filters=512, kernel_size=3,batchnorm=True)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model  \n",
    "\n",
    "# Instantiate the model\n",
    "model = unet(num_classes=NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up early stopping and reduce learning rate on plateau\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='accuracy', \n",
    "    patience=10, \n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='accuracy', \n",
    "    factor=0.8,   \n",
    "    patience=6, \n",
    "    min_lr=0.00000001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpoint for saving model weights\n",
    "checkpoint_filepath = 'checkpoint.model.keras'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='accuracy',  # Monitor validation accuracy for saving the best model\n",
    "    mode='max',  # Save the model with the highest validation accuracy\n",
    "    save_best_only=True  # Only save the best model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compilation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "train_steps = len(x_train)//BATCH_SIZE\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_generator, epochs=EPOCHS, steps_per_epoch=train_steps,callbacks=[early_stopping, reduce_lr, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss and Accuracy Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "# plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Training Loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "# plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Training accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Get true labels from the test generator\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Optional: Print classification report for detailed metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels, predicted_labels, target_names=['Class 0', 'Class 1', 'Class 2', 'Class 3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "sns.heatmap(cm, \n",
    "            annot=True,\n",
    "            fmt='g', \n",
    "            xticklabels=['mf0', 'mf1', 'mf2', 'mf3'],\n",
    "            yticklabels=['mf0', 'mf1', 'mf2', 'mf3'])\n",
    "plt.xlabel('Prediction', fontsize=13)\n",
    "plt.ylabel('Actual', fontsize=13)\n",
    "plt.title('Confusion Matrix', fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_saves/classification_model.keras') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
